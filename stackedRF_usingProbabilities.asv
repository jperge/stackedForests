        %% Stacked Random Forests:
         % Concept: Train an ensemble of random forests on individual time
         % bins across all neurons, then train a final random forest based
         % on the output of the ensemble.
         
         % Each time bin for all units is used to train one random forest
         % The votes of the trees from the forest is converted into
         % probabilities as counts/total number of trees (four probabilites/forest)
         % These probabilities are then used to grow a final random forest. 

        
        load testData
        
        xTrain = X(1:240,:,:);
        xTest1 = X(241:320,:,:);
        xTest2 = X(321:400,:,:);
        
        yTrain = y(1:240);
        yTest1 = y(241:320);
        yTest2 = y(321:400);
        
        nTrees = 500;
        classIDs = unique(y);
        nTest    = length(yTest2);
        
        %% check that data is consistent:
        mod   = classRF_train(squeeze(xTrain(:,5,:)),yTrain,nTrees);
        yHat  = classRF_predict(squeeze(xTest(:,iBin,:)),mod);
        perc = sum( yTest == yHat )/nTest
        %%(if all correct, this should give a result around perc = 0.41)
        
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %% TRAINING
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %% Train first layer of random forests on individual time bins,
        %%and across all neurons:
        clear yHats votes
        [nEx,nBin,nFeat] = size(xTrain);
        unitEstimates = zeros(nEx,nBin*length(classIDs));
        yHats         = zeros(nEx,nBin); %unused, only for program flow checking
        for iBin = 1:nBin
            model{iBin} = classRF_train(squeeze(xTrain(:,iBin,:)),yTrain,nTrees);
            [yHats(:,iBin), votes] = classRF_predict(squeeze(xTest1(:,iBin,:)),model{iBin});
            votes = votes./nTrees;
            unitEstimates(:,1+(iBin-1)*4 : 4+(iBin-1)*4 ) = votes;
        end
        
        %train ultimate random forest using label probabilities generated by the
        %ensemble of random forests:
        model2 = classRF_train(unitEstimates,yTest1,nTrees);
         
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %% TESTING
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %% test two-layer model on test data:
        [nEx,nBin,nFeat] = size(xTest2);
        unitEstimates2 = zeros(nEx,nBin*length(classIDs));
        yHats2         = zeros(nEx,nBin); %unused, only for program flow checking
        perci   = zeros(nBin,1);
        % train random forest (max code)
        for iBin = 1:nBin
            [yHats2(:,iBin), votes]  = classRF_predict(squeeze(xTest2(:,iBin,:)),model{iBin});
            perci(iBin,1) = sum(yHats2(:,iBin) == yTest2)./length(yTest2);
            votes = votes./nTrees;
            unitEstimates2(:,1+(iBin-1)*4 : 4+(iBin-1)*4 ) = votes;
        end
        yHat = classRF_predict(unitEstimates2,model2);

        perc = sum( yTest2 == yHat )/nTest
        %      
