% 9/25
% Tried another approach: 
% Training and testing on the same dataset will produce unrealistic performance that does not generalize (performs poor when tested on new data). 
% --> These unrealistic estimates are not very informative for the ultimate RF. 
% To overcome this, I also trained the forest ensemble on a large chunk of data, then generated probabilities on new data, used these labels to train the ultimate forest, and once trained, run both the ensemble and the ultimate forest on new data (train1, train2, test). 
% --Performance is slightly better, 38%. Still not good enough. 

        %% Stacked Random Forests:
         % Concept: Train an ensemble of random forests on individual time
         % bins across all neurons, then train a final random forest based
         % on the output of the ensemble.
         
         % Each time bin for all units is used to train one random forest
         % The votes of the trees from the forest is converted into
         % probabilities as counts/total number of trees (four probabilites/forest)
         % These probabilities are then used to grow a final random forest. 

        
        load testData
        
        xTrain = X(1:240,:,:);
        xTest1 = X(241:320,:,:);
        xTest2 = X(321:400,:,:);
        
        yTrain = y(1:240);
        yTest1 = y(241:320);
        yTest2 = y(321:400);
        
        nTrees = 500;
        classIDs = unique(y);
        nTest    = length(yTest2);
                
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %% TRAINING
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %% Train first layer of random forests on individual time bins,
        %%and across all neurons:
        clear yHats votes
        [nEx,nBin,nFeat] = size(xTest1);
        unitEstimates = zeros(nEx,nBin*length(classIDs));
        yHats         = zeros(nEx,nBin); %unused, only for program flow checking
        for iBin = 1:nBin
            model{iBin} = classRF_train(squeeze(xTrain(:,iBin,:)),yTrain,nTrees);
            [yHats(:,iBin), votes] = classRF_predict(squeeze(xTest1(:,iBin,:)),model{iBin});
            votes = votes./nTrees;
            unitEstimates(:,1+(iBin-1)*4 : 4+(iBin-1)*4 ) = votes;
        end
        
        %train ultimate random forest using label probabilities generated by the
        %ensemble of random forests:
        model2 = classRF_train(unitEstimates,yTest1,nTrees);
         
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %% TESTING
        %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
        %% test two-layer model on test data:
        [nEx,nBin,nFeat] = size(xTest2);
        unitEstimates2 = zeros(nEx,nBin*length(classIDs));
        yHats2         = zeros(nEx,nBin); %unused, only for program flow checking
        perci   = zeros(nBin,1);
        % train random forest (max code)
        for iBin = 1:nBin
            [yHats2(:,iBin), votes]  = classRF_predict(squeeze(xTest2(:,iBin,:)),model{iBin});
            perci(iBin,1) = sum(yHats2(:,iBin) == yTest2)./length(yTest2);
            votes = votes./nTrees;
            unitEstimates2(:,1+(iBin-1)*4 : 4+(iBin-1)*4 ) = votes;
        end
        yHat = classRF_predict(unitEstimates2,model2);

        perc = sum( yTest2 == yHat )/nTest
        %      
